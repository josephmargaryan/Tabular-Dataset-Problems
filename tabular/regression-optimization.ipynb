{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":80874,"databundleVersionId":8794587,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom math import pi\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.metrics import mean_absolute_percentage_error\nimport torch\nimport torch.nn as nn\nfrom scipy import stats\nimport optuna\nimport xgboost as xgb\nimport catboost as cb\nimport lightgbm\nfrom sklearn.model_selection import cross_val_score","metadata":{"execution":{"iopub.status.busy":"2024-07-26T21:34:13.297646Z","iopub.execute_input":"2024-07-26T21:34:13.298967Z","iopub.status.idle":"2024-07-26T21:34:13.307541Z","shell.execute_reply.started":"2024-07-26T21:34:13.298922Z","shell.execute_reply":"2024-07-26T21:34:13.305977Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-07-26T20:58:05.047269Z","iopub.execute_input":"2024-07-26T20:58:05.047732Z","iopub.status.idle":"2024-07-26T20:58:05.054270Z","shell.execute_reply.started":"2024-07-26T20:58:05.047699Z","shell.execute_reply":"2024-07-26T20:58:05.052793Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/rohlik-orders-forecasting-challenge/train.csv')\ntrain_calendar = pd.read_csv('/kaggle/input/rohlik-orders-forecasting-challenge/train_calendar.csv')\n\ntest = pd.read_csv('/kaggle/input/rohlik-orders-forecasting-challenge/test.csv')\ntest_calendar = pd.read_csv('/kaggle/input/rohlik-orders-forecasting-challenge/test_calendar.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-26T20:30:46.890609Z","iopub.execute_input":"2024-07-26T20:30:46.891068Z","iopub.status.idle":"2024-07-26T20:30:46.999222Z","shell.execute_reply.started":"2024-07-26T20:30:46.891032Z","shell.execute_reply":"2024-07-26T20:30:46.998085Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def align_train_data(train_df, test_df, target_column=None):\n    \"\"\"\n    Align columns of the training DataFrame to match the test DataFrame.\n    \n    Parameters:\n    - train_df: DataFrame containing training data.\n    - test_df: DataFrame containing test data.\n    - target_column: Optional; target column to exclude from alignment.\n    \n    Returns:\n    - Aligned training DataFrame with only columns present in the test DataFrame.\n    \"\"\"\n    # Get columns to keep\n    test_columns = test_df.columns.tolist()\n    \n    # Remove extra columns from the training DataFrame\n    train_columns = [col for col in train_df.columns if col in test_columns]\n    \n    # Ensure the target column is included in the train DataFrame if it's present\n    if target_column and target_column in train_df.columns:\n        train_columns.append(target_column)\n    \n    # Filter and reorder columns\n    aligned_train_df = train_df[train_columns]\n    \n    return aligned_train_df\n\ndef merge_with_calendar(calendar_df, data_df):\n    merged_df = calendar_df.merge(data_df, how='right', on=['date', 'warehouse'])\n    return merged_df\n\n# Preprocess function\ndef preprocess(merged_df):\n    # Fill missing values with 0\n    merged_df.fillna(0, inplace=True)\n\n    # Convert 'date' to datetime\n    merged_df['date'] = pd.to_datetime(merged_df['date'])\n\n    # Generate time-based features\n    merged_df['day_of_week'] = merged_df['date'].dt.dayofweek\n    merged_df['month'] = merged_df['date'].dt.month\n    merged_df['is_weekend'] = merged_df['day_of_week'].isin([5, 6]).astype(int)\n    merged_df['year'] = merged_df['date'].dt.year\n    merged_df['day_of_month'] = merged_df['date'].dt.day\n    merged_df['week_of_year'] = merged_df['date'].dt.isocalendar().week\n    merged_df['quarter'] = merged_df['date'].dt.quarter\n    merged_df['is_start_of_month'] = merged_df['date'].dt.is_month_start.astype(int)\n    merged_df['is_end_of_month'] = merged_df['date'].dt.is_month_end.astype(int)\n    merged_df['is_quarter_start'] = merged_df['date'].dt.is_quarter_start.astype(int)\n    merged_df['is_quarter_end'] = merged_df['date'].dt.is_quarter_end.astype(int)\n\n    # Generate cyclical features\n    merged_df['month_normalized'] = merged_df['month'] / 12\n    merged_df['day_normalized'] = merged_df['day_of_month'] / 31\n    merged_df['month_sin'] = np.sin(2 * np.pi * merged_df['month_normalized'])\n    merged_df['month_cos'] = np.cos(2 * np.pi * merged_df['month_normalized'])\n    merged_df['day_sin'] = np.sin(2 * np.pi * merged_df['day_normalized'])\n    merged_df['day_cos'] = np.cos(2 * np.pi * merged_df['day_normalized'])\n    merged_df.drop(columns=['month_normalized', 'day_normalized'], inplace=True)\n\n    return merged_df\n\ntrain = align_train_data(train, test, target_column='orders')\ntrain_calendar = align_train_data(train_calendar, test_calendar, target_column='orders')\n\n# Merge and preprocess train data\ntrain_merged = merge_with_calendar(train_calendar, train)\ntrain_preprocessed = preprocess(train_merged)\n\n# Merge and preprocess test data\ntest_merged = merge_with_calendar(test_calendar, test)\ntest_preprocessed = preprocess(test_merged)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T20:30:49.077109Z","iopub.execute_input":"2024-07-26T20:30:49.078511Z","iopub.status.idle":"2024-07-26T20:30:49.187122Z","shell.execute_reply.started":"2024-07-26T20:30:49.078433Z","shell.execute_reply":"2024-07-26T20:30:49.185916Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Encode object columns\nobject_cols = [col for col in train_preprocessed.columns if train_preprocessed[col].dtype == 'object' and col != 'id']\nle_dict = {}\n\nfor col in object_cols:\n    le = LabelEncoder()\n    # Combine train and test data for consistent encoding\n    combined_data = pd.concat([train_preprocessed[col], test_preprocessed[col]], axis=0).astype(str)\n    le.fit(combined_data)\n    train_preprocessed[col] = le.transform(train_preprocessed[col].astype(str))\n    test_preprocessed[col] = le.transform(test_preprocessed[col].astype(str))\n    le_dict[col] = le","metadata":{"execution":{"iopub.status.busy":"2024-07-26T20:30:52.704953Z","iopub.execute_input":"2024-07-26T20:30:52.705922Z","iopub.status.idle":"2024-07-26T20:30:52.748234Z","shell.execute_reply.started":"2024-07-26T20:30:52.705875Z","shell.execute_reply":"2024-07-26T20:30:52.746647Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def remove_outliers_zscore(df, group_col, value_col, threshold=3):\n    # Group by the warehouse\n    grouped = df.groupby(group_col)\n    \n    # Initialize an empty DataFrame for filtered data\n    filtered_df = pd.DataFrame()\n\n    for name, group in grouped:\n        # Calculate Z-scores for the group\n        z_scores = np.abs(stats.zscore(group[value_col]))\n        \n        # Filter out outliers\n        group_filtered = group[z_scores < threshold]\n        \n        # Append to the filtered DataFrame\n        filtered_df = pd.concat([filtered_df, group_filtered], ignore_index=True)\n    \n    return filtered_df\n\n# Apply the function\ntrain_filtered_zscore = remove_outliers_zscore(train_preprocessed, 'warehouse', 'orders')\n\ndef remove_outliers_iqr(df, group_col, value_col):\n    # Group by the warehouse\n    grouped = df.groupby(group_col)\n    \n    # Initialize an empty DataFrame for filtered data\n    filtered_df = pd.DataFrame()\n\n    for name, group in grouped:\n        # Calculate Q1 and Q3\n        Q1 = group[value_col].quantile(0.25)\n        Q3 = group[value_col].quantile(0.75)\n        IQR = Q3 - Q1\n        \n        # Define outlier bounds\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        \n        # Filter out outliers\n        group_filtered = group[(group[value_col] >= lower_bound) & (group[value_col] <= upper_bound)]\n        \n        # Append to the filtered DataFrame\n        filtered_df = pd.concat([filtered_df, group_filtered], ignore_index=True)\n    \n    return filtered_df\n\n# Apply the function\ntrain_filtered_iqr = remove_outliers_iqr(train_filtered_zscore, 'warehouse', 'orders')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-26T20:31:15.549869Z","iopub.execute_input":"2024-07-26T20:31:15.550743Z","iopub.status.idle":"2024-07-26T20:31:15.661542Z","shell.execute_reply.started":"2024-07-26T20:31:15.550702Z","shell.execute_reply":"2024-07-26T20:31:15.659920Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Separate features and target in train data\nX = train_filtered_iqr.drop(columns=['orders', 'id', 'date'], axis=1)\ny = train_filtered_iqr['orders']\n\n# Scale the features\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n\n# Prepare test data for submission\ntest_ids = test_preprocessed['id']\ntest = test_preprocessed.drop(columns=['id', 'date'], axis=1)\ntest = scaler.transform(test)","metadata":{"execution":{"iopub.status.busy":"2024-07-26T20:31:19.419658Z","iopub.execute_input":"2024-07-26T20:31:19.420132Z","iopub.status.idle":"2024-07-26T20:31:19.456571Z","shell.execute_reply.started":"2024-07-26T20:31:19.420095Z","shell.execute_reply":"2024-07-26T20:31:19.455492Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def catboost_params(trial):\n    return {\n        'iterations': trial.suggest_int('iterations', 100, 10000),\n        'depth': trial.suggest_int('depth', 1, 16),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 10.0),\n        'border_count': trial.suggest_int('border_count', 1, 255),\n        'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 0.01, 10.0),\n        'random_strength': trial.suggest_loguniform('random_strength', 0.01, 10.0),\n        'rsm': trial.suggest_uniform('rsm', 0.1, 1.0),\n        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 20),\n        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n        'scale_pos_weight': trial.suggest_loguniform('scale_pos_weight', 0.01, 10.0)\n    }\n\ndef lgbm_params(trial):\n    return {\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'max_depth': trial.suggest_int('max_depth', -1, 128),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 10000),\n        'min_split_gain': trial.suggest_loguniform('min_split_gain', 1e-4, 10.0),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 10.0),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 100),\n        'subsample': trial.suggest_uniform('subsample', 0.1, 1.0),\n        'subsample_freq': trial.suggest_int('subsample_freq', 0, 10),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1.0),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 10.0),\n        'scale_pos_weight': trial.suggest_loguniform('scale_pos_weight', 0.01, 10.0)\n    }\n\ndef xgb_params(trial):\n    return {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 10000),\n        'max_depth': trial.suggest_int('max_depth', 1, 16),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n        'subsample': trial.suggest_uniform('subsample', 0.1, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1.0),\n        'colsample_bylevel': trial.suggest_uniform('colsample_bylevel', 0.1, 1.0),\n        'colsample_bynode': trial.suggest_uniform('colsample_bynode', 0.1, 1.0),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 10.0),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 10.0),\n        'gamma': trial.suggest_loguniform('gamma', 1e-4, 10.0),\n        'scale_pos_weight': trial.suggest_loguniform('scale_pos_weight', 0.01, 10.0)\n    }\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective_catboost_cv(trial):\n    params = catboost_params(trial)\n    model = CatBoostRegressor(**params, silent=True)\n    score = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n    mse = -score.mean()\n    return mse\n\nstudy_catboost = optuna.create_study(direction='minimize')\nstudy_catboost.optimize(objective_catboost_cv, n_trials=100)\nprint(\"Best CatBoost params:\", study_catboost.best_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective_lgbm_cv(trial):\n    params = lgbm_params(trial)\n    model = LGBMRegressor(**params)\n    score = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n    mse = -score.mean()\n    return mse\n\nstudy_lgbm = optuna.create_study(direction='minimize')\nstudy_lgbm.optimize(objective_lgbm_cv, n_trials=100)\nprint(\"Best LGBM params:\", study_lgbm.best_params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective_xgb_cv(trial):\n    params = xgb_params(trial)\n    model = XGBRegressor(**params)\n    score = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n    mse = -score.mean()\n    return mse\n\nstudy_xgb = optuna.create_study(direction='minimize')\nstudy_xgb.optimize(objective_xgb_cv, n_trials=100)\nprint(\"Best XGB params:\", study_xgb.best_params)","metadata":{},"execution_count":null,"outputs":[]}]}